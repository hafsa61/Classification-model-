# -*- coding: utf-8 -*-
"""Classification model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gnnmng8HKPrBSt337OgFi3LCFud8885e
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv('train_data.csv')
tf=pd.read_csv('test_data.csv')
tf.head()

y=df['Investment_Failed']
X=df.drop(columns=['Investor_ID','Investment_Failed'])

X.head()
# tf.head()

X['Age']=X['Age'].fillna(X['Age'].mean())
X['Annual_Income']=X['Annual_Income'].fillna(X['Annual_Income'].mean())
X['Investment_Amount']=X['Investment_Amount'].fillna(X['Investment_Amount'].mean())
X['Potential_Return_Rate']=X['Potential_Return_Rate'].fillna(X['Potential_Return_Rate'].mean())
X.isnull().sum()

tf['Age']=tf['Age'].fillna(tf['Age'].mean())
tf['Annual_Income']=tf['Annual_Income'].fillna(tf['Annual_Income'].mean())
tf['Investment_Amount']=tf['Investment_Amount'].fillna(tf['Investment_Amount'].mean())
tf['Potential_Return_Rate']=tf['Potential_Return_Rate'].fillna(tf['Potential_Return_Rate'].mean())
tf.isnull().sum()

X['Number_of_Active_Investments'].value_counts()
tf['Number_of_Active_Investments'].value_counts()

def fill_null_with_random():
    return np.random.randint(1.0, 5.0)
X['Number_of_Active_Investments'] = X['Number_of_Active_Investments'].apply(lambda x: fill_null_with_random() if pd.isnull(x) else x)
X['Number_of_Active_Investments'].isnull().sum()
X['Number_of_Active_Investments'].value_counts()

tf['Number_of_Active_Investments'] = tf['Number_of_Active_Investments'].apply(lambda x: fill_null_with_random() if pd.isnull(x) else x)
tf['Number_of_Active_Investments'].isnull().sum()
tf['Number_of_Active_Investments'].value_counts()

X['Marital_Status'].value_counts()
tf['Marital_Status'].value_counts()

X['Marital_Status']=X['Marital_Status'].str.replace('Div','Divorced')
X['Marital_Status']=X['Marital_Status'].str.replace('Marr','Married')
X['Marital_Status']=X['Marital_Status'].str.replace('Sgl','Single')

tf['Marital_Status']=tf['Marital_Status'].str.replace('Div','Divorced')
tf['Marital_Status']=tf['Marital_Status'].str.replace('Marr','Married')
tf['Marital_Status']=tf['Marital_Status'].str.replace('Sgl','Single')

X['Marital_Status']=X['Marital_Status'].str.replace('Marriedied','Married')
X['Marital_Status']=X['Marital_Status'].str.replace('Divorcedorced','Divorced')
X['Marital_Status'].value_counts()

tf['Marital_Status']=tf['Marital_Status'].str.replace('Marriedied','Married')
tf['Marital_Status']=tf['Marital_Status'].str.replace('Divorcedorced','Divorced')
tf['Marital_Status'].value_counts()

from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder = OrdinalEncoder()
X[['Marital_Status']] = ordinal_encoder.fit_transform(X[['Marital_Status']])
X[['Marital_Status']].value_counts()

tf[['Marital_Status']] = ordinal_encoder.fit_transform(tf[['Marital_Status']])
tf[['Marital_Status']].value_counts()

def fill_null_with_random():
    return np.random.randint(0.0, 3.0)
X['Marital_Status'] = X['Marital_Status'].apply(lambda x: fill_null_with_random() if pd.isnull(x) else x)
X['Marital_Status'].isnull().sum()
X['Marital_Status'].value_counts()

tf['Marital_Status'] = tf['Marital_Status'].apply(lambda x: fill_null_with_random() if pd.isnull(x) else x)
tf['Marital_Status'].isnull().sum()
tf['Marital_Status'].value_counts()

X['Has_Investment_Advisor']=X['Has_Investment_Advisor'].replace(True,1.0)
X['Has_Investment_Advisor']=X['Has_Investment_Advisor'].replace(False,0.0)
X['Has_Investment_Advisor'].value_counts()

tf['Has_Investment_Advisor']=tf['Has_Investment_Advisor'].replace(True,1.0)
tf['Has_Investment_Advisor']=tf['Has_Investment_Advisor'].replace(False,0.0)
tf['Has_Investment_Advisor'].value_counts()

def fill_null_with_random():
    return np.random.randint(0.0, 2.0)
X['Has_Investment_Advisor'] = X['Has_Investment_Advisor'].apply(lambda x: fill_null_with_random() if pd.isnull(x) else x)
X['Has_Investment_Advisor'].isnull().sum()
X['Has_Investment_Advisor'].value_counts()

tf['Has_Investment_Advisor'] = tf['Has_Investment_Advisor'].apply(lambda x: fill_null_with_random() if pd.isnull(x) else x)
tf['Has_Investment_Advisor'].isnull().sum()
tf['Has_Investment_Advisor'].value_counts()

X.head()

X['Employment_Status']=X['Employment_Status'].str.replace('Self','Self-employed')
X['Employment_Status']=X['Employment_Status'].str.replace('Unem','Unemployed')
X['Employment_Status']=X['Employment_Status'].str.replace('Full','Full-time')
X['Employment_Status']=X['Employment_Status'].str.replace('Part','Part-time')

tf['Employment_Status']=tf['Employment_Status'].str.replace('Self','Self-employed')
tf['Employment_Status']=tf['Employment_Status'].str.replace('Unem','Unemployed')
tf['Employment_Status']=tf['Employment_Status'].str.replace('Full','Full-time')
tf['Employment_Status']=tf['Employment_Status'].str.replace('Part','Part-time')

X['Employment_Status']=X['Employment_Status'].str.replace('Part-time-time','Part-time')
X['Employment_Status']=X['Employment_Status'].str.replace('Full-time-time','Full-time')
X['Employment_Status']=X['Employment_Status'].str.replace('Self-employed-employed','Self-employed')
X['Employment_Status']=X['Employment_Status'].str.replace('Unemployedployed','Unemployed')
X['Employment_Status'].value_counts()

tf['Employment_Status']=tf['Employment_Status'].str.replace('Part-time-time','Part-time')
tf['Employment_Status']=tf['Employment_Status'].str.replace('Full-time-time','Full-time')
tf['Employment_Status']=tf['Employment_Status'].str.replace('Self-employed-employed','Self-employed')
tf['Employment_Status']=tf['Employment_Status'].str.replace('Unemployedployed','Unemployed')
tf['Employment_Status'].value_counts()

X['Owns_Property']=X['Owns_Property'].str.replace('Maybe','Others')
X['Owns_Property']=X['Owns_Property'].str.replace('Unknown','Others')
X['Owns_Property']=X['Owns_Property'].str.replace('Ye','Yes')
X['Owns_Property']=X['Owns_Property'].str.replace('no','No')
X['Owns_Property']=X['Owns_Property'].str.replace('NO','No')
X['Owns_Property']=X['Owns_Property'].str.replace('yes','Yes')
X['Owns_Property']=X['Owns_Property'].str.replace('YE','Yes')
X['Owns_Property']=X['Owns_Property'].str.replace('ye','Yes')

tf['Owns_Property']=tf['Owns_Property'].str.replace('Maybe','Others')
tf['Owns_Property']=tf['Owns_Property'].str.replace('Unknown','Others')
tf['Owns_Property']=tf['Owns_Property'].str.replace('Ye','Yes')
tf['Owns_Property']=tf['Owns_Property'].str.replace('no','No')
tf['Owns_Property']=tf['Owns_Property'].str.replace('NO','No')
tf['Owns_Property']=tf['Owns_Property'].str.replace('yes','Yes')
tf['Owns_Property']=tf['Owns_Property'].str.replace('YE','Yes')
tf['Owns_Property']=tf['Owns_Property'].str.replace('ye','Yes')

X['Owns_Property']=X['Owns_Property'].str.replace('Yess','Yes')
X['Owns_Property']=X['Owns_Property'].str.replace('YesS','Yes')
X['Owns_Property'].value_counts()

tf['Owns_Property']=tf['Owns_Property'].str.replace('Yess','Yes')
tf['Owns_Property']=tf['Owns_Property'].str.replace('YesS','Yes')
tf['Owns_Property'].value_counts()

X['Has_Dependents'].value_counts()

X['Has_Dependents']=X['Has_Dependents'].str.replace('Maybe','Others')
X['Has_Dependents']=X['Has_Dependents'].str.replace('Unknown','Others')
X['Has_Dependents']=X['Has_Dependents'].str.replace('Ye','Yes')
X['Has_Dependents']=X['Has_Dependents'].str.replace('no','No')
X['Has_Dependents']=X['Has_Dependents'].str.replace('NO','No')
X['Has_Dependents']=X['Has_Dependents'].str.replace('yes','Yes')
X['Has_Dependents']=X['Has_Dependents'].str.replace('YE','Yes')
X['Has_Dependents']=X['Has_Dependents'].str.replace('ye','Yes')

tf['Has_Dependents']=tf['Has_Dependents'].str.replace('Maybe','Others')
tf['Has_Dependents']=tf['Has_Dependents'].str.replace('Unknown','Others')
tf['Has_Dependents']=tf['Has_Dependents'].str.replace('Ye','Yes')
tf['Has_Dependents']=tf['Has_Dependents'].str.replace('no','No')
tf['Has_Dependents']=tf['Has_Dependents'].str.replace('NO','No')
tf['Has_Dependents']=tf['Has_Dependents'].str.replace('yes','Yes')
tf['Has_Dependents']=tf['Has_Dependents'].str.replace('YE','Yes')
tf['Has_Dependents']=tf['Has_Dependents'].str.replace('ye','Yes')

X['Has_Dependents']=X['Has_Dependents'].str.replace('Yess','Yes')
X['Has_Dependents']=X['Has_Dependents'].str.replace('YesS','Yes')
X['Has_Dependents'].value_counts()

tf['Has_Dependents']=tf['Has_Dependents'].str.replace('Yess','Yes')
tf['Has_Dependents']=tf['Has_Dependents'].str.replace('YesS','Yes')
tf['Has_Dependents'].value_counts()

from sklearn.model_selection import KFold
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

svm_model = SVC(kernel='rbf')
logistic_model = LogisticRegression(max_iter=1000)
mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam')
xgb_model = XGBClassifier()

categorical_cols = ['Has_Investment_Advisor', 'Number_of_Active_Investments','Education','Employment_Status','Marital_Status','Owns_Property','Has_Dependents','Investment_Sector']


X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)
X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols)

n_splits = 5
kf = KFold(n_splits=n_splits)

meta_train = np.zeros((len(X_train), 4))
meta_test = np.zeros((len(X_test), 4))

X_train_encoded = X_train_encoded.values
X_test_encoded = X_test_encoded.values
y_train = y_train.values

for train_index, val_index in kf.split(X_train_encoded):
    X_train_fold, X_val_fold = X_train_encoded[train_index], X_train_encoded[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    svm_model.fit(X_train_fold, y_train_fold)
    logistic_model.fit(X_train_fold, y_train_fold)
    mlp_model.fit(X_train_fold, y_train_fold)
    xgb_model.fit(X_train_fold, y_train_fold)

    meta_train[val_index, 0] = svm_model.predict(X_val_fold)
    meta_train[val_index, 1] = logistic_model.predict(X_val_fold)
    meta_train[val_index, 2] = mlp_model.predict(X_val_fold)
    meta_train[val_index, 3] = xgb_model.predict(X_val_fold)

    meta_test += np.column_stack((svm_model.predict(X_test_encoded),
                                   logistic_model.predict(X_test_encoded),
                                   mlp_model.predict(X_test_encoded),
                                   xgb_model.predict(X_test_encoded)))


meta_test /= n_splits

meta_model = XGBClassifier()
meta_model.fit(meta_train, y_train)

predictions = meta_model.predict(meta_test)

from sklearn.model_selection import StratifiedKFold
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import numpy as np
import pandas as pd
from joblib import Parallel, delayed

# Assuming X, y are defined

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
svm_model = SVC(kernel='rbf', probability=True)
logistic_model = LogisticRegression(max_iter=1000)
mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam')
xgb_model = XGBClassifier(n_jobs=-1)

# Define categorical columns
categorical_cols = ['Has_Investment_Advisor', 'Number_of_Active_Investments','Education','Employment_Status','Marital_Status','Owns_Property','Has_Dependents','Investment_Sector']

# One-hot encode categorical variables
encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
X_train_encoded = encoder.fit_transform(X_train[categorical_cols])
X_test_encoded = encoder.transform(X_test[categorical_cols])

# Define number of splits for cross-validation
n_splits = 3

# Initialize StratifiedKFold
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize arrays to store meta features
meta_train = np.zeros((len(X_train), 4))
meta_test = np.zeros((len(X_test), 4))

# Function to train model and make predictions
def train_and_predict(model, X_train_fold, y_train_fold, X_val_fold, X_test_encoded):
    model.fit(X_train_fold, y_train_fold)
    return model.predict(X_val_fold), model.predict(X_test_encoded).reshape(-1, 1)

# Iterate over folds
for i, (train_index, val_index) in enumerate(kf.split(X_train_encoded, y_train)):
    X_train_fold, X_val_fold = X_train_encoded[train_index], X_train_encoded[val_index]
    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

    # Parallelize the training and prediction steps for base models
    base_model_predictions = Parallel(n_jobs=-1)(
        delayed(train_and_predict)(model, X_train_fold, y_train_fold, X_val_fold, X_test_encoded) for model in [svm_model, logistic_model, mlp_model, xgb_model]
    )

    # Fill in meta features
    for j, (val_pred, test_pred) in enumerate(base_model_predictions):
        meta_train[val_index, j] = val_pred
        meta_test[:, j] += test_pred.flatten()

meta_test /= n_splits  # Take the average for meta test predictions

# Train meta-model
meta_model = XGBClassifier(n_jobs=-1)
meta_model.fit(meta_train, y_train)

# Make predictions using meta-model
predictions = meta_model.predict(meta_test)

tf=tf.drop(columns=['Investor_ID'])

from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
import pandas as pd


X_train= X
y_train= y
X_test=tf

categorical_cols = ['Has_Investment_Advisor', 'Number_of_Active_Investments','Education','Employment_Status','Marital_Status','Owns_Property','Has_Dependents','Investment_Sector']

X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)
X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols)

xgb_model = XGBClassifier(n_jobs=-1)

xgb_model.fit(X_train_encoded, y_train)

predictions = xgb_model.predict(X_test_encoded)

# accuracy = (predictions == y_test).mean()
# print("Accuracy:", accuracy)

from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(xgb_model, X_train_encoded, y_train, cv=5, scoring='accuracy')
print("Cross-validation scores:", cv_scores)
print("Mean accuracy:", cv_scores.mean())

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)

df.head()